import pytest

from scispacy.custom_sentence_segmenter import merge_segments

TEST_CASES = [("LSTM networks, which we preview in Sec. 2, have been successfully", ["LSTM networks, which we preview in Sec. 2, have been successfully"]),
              ("When the tree is simply a chain, both Eqs. 2–8 and Eqs. 9–14 reduce to the standard LSTM transitions, Eqs. 1.", ["When the tree is simply a chain, both Eqs. 2–8 and Eqs. 9–14 reduce to the standard LSTM transitions, Eqs. 1."]),
              ("We used fluorescence time-lapse microscopy (Fig. 1D; fig. S1 and movies S1 and S2) and computational", ["We used fluorescence time-lapse microscopy (Fig. 1D; fig. S1 and movies S1 and S2) and computational"]),
              ("Hill functions indeed fit the data well (Fig. 3A and Table 1).", ["Hill functions indeed fit the data well (Fig. 3A and Table 1)."]),
              ('In order to produce sentence representations that fully capture the semantics of natural language, order-insensitive models are insufficient due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure (e.g., “cats climb trees” vs. “trees climb cats”).', ['In order to produce sentence representations that fully capture the semantics of natural language, order-insensitive models are insufficient due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure (e.g., “cats climb trees” vs. “trees climb cats”).']),
              ("There is an average exact sparsity (fraction of zeros) of the hidden layers of 83.40% on MNIST and 72.00% on CIFAR10. Figure 3 (left) provides a better understanding of the influence of sparsity.", ["There is an average exact sparsity (fraction of zeros) of the hidden layers of 83.40% on MNIST and 72.00% on CIFAR10.", "Figure 3 (left) provides a better understanding of the influence of sparsity."]),
              ("Sparsity has become a concept of interest, not only in computational neuroscience and machine learning but also in statistics and signal processing (Candes and Tao, 2005). It was first introduced in computational neuroscience in the context of sparse coding in the visual system (Olshausen and Field, 1997).", ["Sparsity has become a concept of interest, not only in computational neuroscience and machine learning but also in statistics and signal processing (Candes and Tao, 2005).", "It was first introduced in computational neuroscience in the context of sparse coding in the visual system (Olshausen and Field, 1997)."]),
              ("1) The first item. 2) The second item.", ["1) The first item.", "2) The second item."]),
              ("two of these stages (in areas V1 and V2 of visual cortex) (Lee et al., 2008), and that they", ["two of these stages (in areas V1 and V2 of visual cortex) (Lee et al., 2008), and that they"]),
              ("all neu-\nrons fire at", ["all neu-\nrons fire at"]),
              ("the support of the Defense Advanced Resarch Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract", ["the support of the Defense Advanced Resarch Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract"]),
              ("While proprietary environments such as Microsoft Robotics Studio [9] and Webots [10] have many commendable attributes, we feel there is no substitute for a fully open platform.", ["While proprietary environments such as Microsoft Robotics Studio [9] and Webots [10] have many commendable attributes, we feel there is no substitute for a fully open platform."]),
              ("We first produce sentence representations hL and hR for each sentence in the pair using a Tree-LSTM model over each sentence’s parse tree.", ["We first produce sentence representations hL and hR for each sentence in the pair using a Tree-LSTM model over each sentence’s parse tree."]),
              ("LSTM networks, which we review in Sec. 2, have been successfully applied to a variety of sequence modeling and prediction tasks, notably machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), speech recognition (Graves et al., 2013), image caption generation (Vinyals et al., 2014), and program execution (Zaremba and Sutskever, 2014).", ["LSTM networks, which we review in Sec. 2, have been successfully applied to a variety of sequence modeling and prediction tasks, notably machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), speech recognition (Graves et al., 2013), image caption generation (Vinyals et al., 2014), and program execution (Zaremba and Sutskever, 2014)."]),
              ("1 Introduction\n\nMost models for distributed representations of phrases and sentences—that is, models where realvalued vectors are used to represent meaning—fall into one of three classes: bag-of-words models, sequence models, and tree-structured models.", ["1 Introduction\n\n", "Most models for distributed representations of phrases and sentences—that is, models where realvalued vectors are used to represent meaning—fall into one of three classes: bag-of-words models, sequence models, and tree-structured models."]),
              ("In this section, we will elaborate these philosophies and shows how they influenced the design and implementation of ROS.\n\nA. Peer-to-Peer\n\nA system built using ROS consists of a number of processes, potentially on a number of different", ["In this section, we will elaborate these philosophies and shows how they influenced the design and implementation of ROS.\n\n", "A. Peer-to-Peer\n\n", "A system built using ROS consists of a number of processes, potentially on a number of different"]),
              ("\n\n2 Long Short-Term Memory Networks\n\n\n\n2.1 Overview\n\nRecurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the recursive application of a transition function on a hidden state vector ht.", ["\n\n2 Long Short-Term Memory Networks\n\n\n\n", "2.1 Overview\n\n", "Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the recursive application of a transition function on a hidden state vector ht."]),
              ("In order to address all three aspects, it is necessary to observe gene regulation in individual cells over time. Therefore, we built Bl-cascade[ strains of Escherichia coli, containing the l repressor and a downstream gene, such that both the amount of the repressor protein and the rate of expression of its target gene could be monitored simultaneously in individual cells (Fig. 1B). These strains incorporate a yellow fluorescent repressor fusion protein (cI-yfp) and a chromosomally integrated target promoter (P R ) controlling cyan fluorescent protein (cfp).", ["In order to address all three aspects, it is necessary to observe gene regulation in individual cells over time.", "Therefore, we built Bl-cascade[ strains of Escherichia coli, containing the l repressor and a downstream gene, such that both the amount of the repressor protein and the rate of expression of its target gene could be monitored simultaneously in individual cells (Fig. 1B).", "These strains incorporate a yellow fluorescent repressor fusion protein (cI-yfp) and a chromosomally integrated target promoter (P R ) controlling cyan fluorescent protein (cfp)."]),
              ("This is a sentence. (This is an interjected sentence.) This is also a sentence.", ["This is a sentence.", "(This is an interjected sentence.)", "This is also a sentence."]),
              ("Thus, we first compute EMC 3 's response time-i.e., the duration from the initial of a call (from/to a participant in the target region) to the time when the decision of task assignment is made; and then, based on the computed response time, we estimate EMC 3 maximum throughput [28]-i.e., the maximum number of mobile users allowed in the MCS system. EMC 3 algorithm is implemented with the Java SE platform and is running on a Java HotSpot(TM) 64-Bit Server VM; and the implementation details are given in Appendix, available in the online supplemental material.", ["Thus, we first compute EMC 3 's response time-i.e., the duration from the initial of a call (from/to a participant in the target region) to the time when the decision of task assignment is made; and then, based on the computed response time, we estimate EMC 3 maximum throughput [28]-i.e., the maximum number of mobile users allowed in the MCS system.", "EMC 3 algorithm is implemented with the Java SE platform and is running on a Java HotSpot(TM) 64-Bit Server VM; and the implementation details are given in Appendix, available in the online supplemental material."])
             ]

@pytest.mark.parametrize('text,expected_sents', TEST_CASES)
def test_custom_segmentation(en_with_combined_rule_tokenizer_and_segmenter_fixture, remove_new_lines_fixture, text, expected_sents):
    doc = en_with_combined_rule_tokenizer_and_segmenter_fixture(text)
    sents = [s.text for s in doc.sents]
    assert sents == expected_sents

def test_merge_segment():
    segments = ["This sentence mentions Eqs.", "1-4 and should not be split."]
    merged_segments = merge_segments(segments)
    assert len(merged_segments) == 1

    segments = ["It also has a sentence before it.", "This sentence mentions Eqs.", "1-4 and should not be split.", "It also has another sentence after it."]
    merged_segments = merge_segments(segments)
    assert len(merged_segments) == 3

    segments = ["This sentence ends with part an abbreviation that is part of a word material.", "It also has another sentence after it."]
    merged_segments = merge_segments(segments)
    assert len(merged_segments) == 2

    segments = ["This sentence is the last segment and ends with an abbreviation that is part of a word material."]
    merged_segments = merge_segments(segments)
    assert len(merged_segments) == 1